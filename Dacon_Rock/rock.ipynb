{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13c62eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision.models import ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10bf5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(img, target_size=256):\n",
    "    w, h = img.size\n",
    "    ratio = target_size / h if w > h else target_size / w\n",
    "    return img.resize((int(w*ratio), int(h*ratio)))\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    \"train_val_ratio\" : 0.9,\n",
    "    \"batch_size\" : 350,\n",
    "    \"learning_rate\" : 0.0001,\n",
    "    \"epochs\" : 3,\n",
    "    \"transform\" : transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(resize),\n",
    "            # transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48235, 0.45882, 0.40784],\n",
    "                std=[1.0/255.0, 1.0/255.0, 1.0/255.0]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "dataset = ImageFolder(\"./data/train\", transform=hyperparams[\"transform\"])\n",
    "\n",
    "# Train, Val 데이터 분리\n",
    "train_size = int(len(dataset) * hyperparams[\"train_val_ratio\"])\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Train, Val 데이터 로더 정의\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True, drop_last=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29e0b105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /Users/simon/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 60.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
    "model.eval().to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(resize),\n",
    "    # transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.48235, 0.45882, 0.40784],\n",
    "        std=[1.0/255.0, 1.0/255.0, 1.0/255.0]\n",
    "    )\n",
    "])\n",
    "\n",
    "test_dir = \"./data/test\"\n",
    "image_files = sorted([f for f in os.listdir(test_dir) if f.endswith(\".jpg\")])\n",
    "\n",
    "results = []\n",
    "\n",
    "for fname in image_files:\n",
    "    path = os.path.join(test_dir, fname)\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        pred = torch.argmax(output, dim=1).item()\n",
    "        pred_label = dataset.classes[pred]\n",
    "\n",
    "\n",
    "    image_id = os.path.splitext(fname)[0]\n",
    "    results.append((image_id, pred_label))\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"ID\", \"rock_type\"])\n",
    "\n",
    "\n",
    "sub_path = \"./rock_submission.csv\" \n",
    "if os.path.exists(sub_path):\n",
    "    os.remove(sub_path)\n",
    "df.to_csv(sub_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb561e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
